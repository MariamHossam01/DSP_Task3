{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If true, the WAV files will be read and their features will be saved in the CSV files\n",
    "# As this is the most time consuming task, only enable it if you don't have the CSV files yet\n",
    "CREATE_CSV_FILES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the names of the CSV files\n",
    "TRAIN_CSV_FILE = \"train.csv\"\n",
    "TEST_CSV_FILE = \"test.csv\"\n",
    "MORE_TRAIN_CSV_FILE = \"more_train.csv\"\n",
    "MORE_TEST_CSV_FILE = \"more_test.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "FIUhv3GI8FOF",
    "outputId": "5adb2545-0d54-4494-c1bb-6fbb9f69ed45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features of the files in the folder recordings/close will be saved to train.csv\n",
      "The features of the files in the folder recordings/open will be saved to train.csv\n",
      "The features of the files in the folder recordings/open will be saved to test.csv\n",
      "CSV files are created\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import librosa\n",
    "import csv\n",
    "import os\n",
    "import chromaFeatures \n",
    "import librosa.display\n",
    "if(os.path.exists(TRAIN_CSV_FILE) and os.path.isfile(TRAIN_CSV_FILE)):\n",
    "    os.remove(TRAIN_CSV_FILE)\n",
    "if(os.path.exists(TEST_CSV_FILE) and os.path.isfile(TEST_CSV_FILE)):\n",
    "    os.remove(TEST_CSV_FILE)\n",
    "def extractWavFeatures(soundFilesFolder, csvFileName,label):\n",
    "    print(\"The features of the files in the folder \"+soundFilesFolder+\" will be saved to \"+csvFileName)\n",
    "    header = 'filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate '\n",
    "    # header = 'filename '\n",
    "    for i in range(1, 21):\n",
    "        header += f'mfcc{i} '\n",
    "    header += 'label '\n",
    "    header = header.split()\n",
    "    if not os.path.exists(csvFileName):\n",
    "        file = open(csvFileName, 'a', newline='')\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "    else:\n",
    "        file = open(csvFileName, 'a', newline='')\n",
    "        writer = csv.writer(file)\n",
    "    genres = '1 2 3 4 5 6 7 8 9 0'.split()\n",
    "    for filename in os.listdir(soundFilesFolder):\n",
    "        number = f'{soundFilesFolder}/{filename}'\n",
    "        y, sr = librosa.load(number, mono=True, duration=30)\n",
    "        # remove leading and trailing silence\n",
    "        y, index = librosa.effects.trim(y)\n",
    "        # chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        chroma_stft = chromaFeatures.chroma_stft(y=y, sr=sr)\n",
    "        rmse = librosa.feature.rms(y=y)\n",
    "        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'\n",
    "        for e in mfcc:\n",
    "            to_append += f' {np.mean(e)}'\n",
    "        to_append+=f' {label}'\n",
    "        writer.writerow(to_append.split())\n",
    "        # writer.writerow(to_append.split())\n",
    "    file.close()\n",
    "\n",
    "if (CREATE_CSV_FILES == True):\n",
    "    extractWavFeatures(\"recordings/close\", TRAIN_CSV_FILE,1)\n",
    "    extractWavFeatures(\"recordings/open\", TRAIN_CSV_FILE,0)\n",
    "    extractWavFeatures(\"recordings/open\", TEST_CSV_FILE,0)\n",
    "    print(\"CSV files are created\")\n",
    "else:\n",
    "    print(\"CSV files creation is skipped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melspec_mean_list=[]\n",
    "# melspec_var_list=[]\n",
    "# for i in range(1,16):\n",
    "#     y, sr = librosa.load(f'..\\\\data\\\\recordings\\\\Dina\\\\open\\\\h_open_({i}).wav', mono=True, duration=30)\n",
    "#     # this is the number of samples in a window per fft\n",
    "#     # n_fft = 2048\n",
    "#     # # The amount of samples we are shifting after each fft\n",
    "#     # hop_length = 512\n",
    "#     mel_signal = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "#     melspec_mean = np.mean(mel_signal)\n",
    "#     melspec_var = np.var(mel_signal)\n",
    "#     melspec_mean_list.append(melspec_mean)\n",
    "#     melspec_var_list.append(melspec_var)\n",
    "#     # print(f'mean{i}:{melspec_mean}')\n",
    "#     # print(f'var{i}:{melspec_var}')\n",
    "#     # spectrogram = np.abs(mel_signal)\n",
    "#     # power_to_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "#     # librosa.display.specshow(power_to_db, sr=sr, x_axis='time', y_axis='mel', cmap='magma', hop_length=hop_length)\n",
    "# print(f'mean_max{max(melspec_mean_list)},mean_min:{min(melspec_mean_list)},mean_mean{np.mean(melspec_mean_list)}')\n",
    "# print(f'var_max{max(melspec_var_list)},var_min:{min(melspec_var_list)},var_mean{np.mean(melspec_var_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # melspec_mean_list=[]\n",
    "# # melspec_var_list=[]\n",
    "# # for i in range(1,16):\n",
    "# y, sr = librosa.load(f'..\\\\data\\\\recordings\\\\Mariam\\\\close\\\\b_close_(1).wav', mono=True, duration=30)\n",
    "# # this is the number of samples in a window per fft\n",
    "# # n_fft = 2048\n",
    "# # # The amount of samples we are shifting after each fft\n",
    "# # hop_length = 512\n",
    "# mel_signal = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "# melspec_mean = np.mean(mel_signal)\n",
    "# melspec_var = np.var(mel_signal)\n",
    "# melspec_mean_list.append(melspec_mean)\n",
    "# melspec_var_list.append(melspec_var)\n",
    "# # print(f'mean{i}:{melspec_mean}')\n",
    "# # print(f'var{i}:{melspec_var}')\n",
    "# spectrogram = np.abs(mel_signal)\n",
    "# power_to_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "# librosa.display.specshow(power_to_db, sr=sr, x_axis='time', y_axis='mel', cmap='magma', hop_length=hop_length)\n",
    "# # print(f'mean_max{max(melspec_mean_list)},mean_min:{min(melspec_mean_list)},mean_mean{np.mean(melspec_mean_list)}')\n",
    "# # print(f'var_max{max(melspec_var_list)},var_min:{min(melspec_var_list)},var_mean{np.mean(melspec_var_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y, sr = librosa.load('..\\\\recordings\\\\test\\\\close(13).wav', mono=True, duration=30)\n",
    "# mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "# mfcc\n",
    "# mfcc_list=[]\n",
    "# for e in mfcc:\n",
    "#             mfcc_list.append(e)\n",
    "# print(len(mfcc_list))\n",
    "# print(mfcc_list)\n",
    "# # mfcc.shape\n",
    "\n",
    "\n",
    "\n",
    "# import freature_ex as ex\n",
    "\n",
    "# mfcc=ex.get_mfcc(y,sr)\n",
    "# mfcc_list=[]\n",
    "# for e in mfcc:\n",
    "#             mfcc_list.append(e)\n",
    "# print(len(mfcc_list))\n",
    "# print(mfcc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y, sr = librosa.load('..\\\\recordings\\\\test\\\\close(13).wav', mono=True, duration=30)\n",
    "# op=np.mean(librosa.feature.chroma_stft(y=y,sr=sr))\n",
    "# op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chromaFeatures as ft \n",
    "# op= np.mean(ft.chroma_stft(y=y,sr=sr))\n",
    "# op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "colab_type": "code",
    "id": "_TZXrWYiNqCj",
    "outputId": "7d1d926e-64fa-4855-df66-207a97778915"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.csv will be preprocessed\n",
      "Preprocessing is finished\n",
      "   chroma_stft      rmse  spectral_centroid  spectral_bandwidth      rolloff  \\\n",
      "0     0.396980  0.011294        1375.395684         1755.537372  2944.424484   \n",
      "1     0.405457  0.009537        1379.121148         1757.046968  2879.642512   \n",
      "2     0.422009  0.011632        1119.376006         1623.819217  2313.906912   \n",
      "3     0.407065  0.009828        1166.363153         1528.011921  2343.564131   \n",
      "4     0.450757  0.007407        1194.655143         1681.378609  2575.949598   \n",
      "\n",
      "   zero_crossing_rate       mfcc1       mfcc2      mfcc3      mfcc4  ...  \\\n",
      "0            0.077564 -564.512695  107.655190   8.614992  19.808443  ...   \n",
      "1            0.076649 -596.777100  110.699188  16.220989  29.477320  ...   \n",
      "2            0.046618 -576.227600  116.779678  12.934498  24.658026  ...   \n",
      "3            0.065881 -585.294495  126.129944  24.237478  32.909943  ...   \n",
      "4            0.059592 -620.200562  117.409187  20.870218  20.508194  ...   \n",
      "\n",
      "     mfcc12    mfcc13    mfcc14    mfcc15    mfcc16    mfcc17    mfcc18  \\\n",
      "0  4.634001  3.520356  6.777761  3.788116 -5.224450  3.283867 -2.676502   \n",
      "1  6.168213 -2.191288 -4.505795  3.204891 -2.964992  5.152459 -0.360222   \n",
      "2  7.947334  2.546184 -1.708121 -1.993114  0.838701  6.571207 -3.118687   \n",
      "3  2.661880 -3.086222 -4.945445  4.069139 -0.743327 -0.404286  1.997896   \n",
      "4  4.480309  4.203310  0.335586  4.308993  3.168963  3.801324 -4.585912   \n",
      "\n",
      "     mfcc19    mfcc20  label  \n",
      "0 -2.609690 -6.555895      1  \n",
      "1 -3.532550 -5.576309      1  \n",
      "2 -2.019080 -7.327988      1  \n",
      "3  4.884244 -6.720134      1  \n",
      "4 -3.067912 -4.226863      1  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "test.csv will be preprocessed\n",
      "Preprocessing is finished\n",
      "   chroma_stft      rmse  spectral_centroid  spectral_bandwidth      rolloff  \\\n",
      "0     0.472431  0.011665        1396.440106         1711.174845  2841.718207   \n",
      "1     0.407049  0.042138        1085.925250         1486.092808  2028.533635   \n",
      "2     0.373579  0.017066        1178.691303         1522.661807  2300.800323   \n",
      "3     0.422417  0.011613        1223.780849         1484.713954  2360.113315   \n",
      "4     0.447885  0.015438        1306.245087         1721.943742  2670.258853   \n",
      "\n",
      "   zero_crossing_rate       mfcc1       mfcc2      mfcc3      mfcc4  ...  \\\n",
      "0            0.089928 -585.462524  101.576004  15.595903  14.595278  ...   \n",
      "1            0.055576 -441.684601  100.648849   3.834749  15.360082  ...   \n",
      "2            0.063614 -534.316101  101.642128  12.269768  11.262321  ...   \n",
      "3            0.071521 -580.873962  122.117088  15.391713  19.690872  ...   \n",
      "4            0.070814 -536.536316   77.453041   9.865512  14.247538  ...   \n",
      "\n",
      "     mfcc12    mfcc13    mfcc14    mfcc15    mfcc16    mfcc17    mfcc18  \\\n",
      "0  4.741848  1.626162  1.420287  5.759628  0.617246  2.946568 -1.037910   \n",
      "1  4.911519  0.534007  1.016140  9.854089 -1.445153 -3.275455 -1.091314   \n",
      "2  4.143373 -0.545828  3.534702  6.288830 -0.846061  1.967099 -0.270130   \n",
      "3  2.027533  1.490679  6.036329  8.372940 -2.374796  2.119190 -3.562642   \n",
      "4  3.475916  2.847411  2.320698  3.487839 -2.308565 -0.309327 -0.727287   \n",
      "\n",
      "     mfcc19    mfcc20  label  \n",
      "0 -0.438018 -4.450876      0  \n",
      "1 -3.345469 -5.651415      0  \n",
      "2  1.044978 -2.637393      0  \n",
      "3 -1.847220 -3.751944      0  \n",
      "4 -2.788883 -1.487146      0  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_20628\\1104326005.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(csvFileName, error_bad_lines=False)\n",
      "Skipping line 71: expected 28 fields, saw 29\n",
      "Skipping line 72: expected 28 fields, saw 29\n",
      "Skipping line 73: expected 28 fields, saw 29\n",
      "Skipping line 74: expected 28 fields, saw 29\n",
      "Skipping line 75: expected 28 fields, saw 29\n",
      "Skipping line 76: expected 28 fields, saw 29\n",
      "Skipping line 77: expected 28 fields, saw 29\n",
      "Skipping line 78: expected 28 fields, saw 29\n",
      "Skipping line 79: expected 28 fields, saw 29\n",
      "Skipping line 80: expected 28 fields, saw 29\n",
      "Skipping line 81: expected 28 fields, saw 29\n",
      "Skipping line 82: expected 28 fields, saw 29\n",
      "Skipping line 83: expected 28 fields, saw 29\n",
      "Skipping line 84: expected 28 fields, saw 29\n",
      "Skipping line 85: expected 28 fields, saw 29\n",
      "Skipping line 86: expected 28 fields, saw 29\n",
      "Skipping line 87: expected 28 fields, saw 29\n",
      "Skipping line 88: expected 28 fields, saw 29\n",
      "Skipping line 89: expected 28 fields, saw 29\n",
      "\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_20628\\1104326005.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(csvFileName, error_bad_lines=False)\n",
      "Skipping line 17: expected 28 fields, saw 29\n",
      "Skipping line 18: expected 28 fields, saw 29\n",
      "Skipping line 19: expected 28 fields, saw 29\n",
      "Skipping line 20: expected 28 fields, saw 29\n",
      "Skipping line 21: expected 28 fields, saw 29\n",
      "Skipping line 22: expected 28 fields, saw 29\n",
      "Skipping line 23: expected 28 fields, saw 29\n",
      "Skipping line 24: expected 28 fields, saw 29\n",
      "Skipping line 25: expected 28 fields, saw 29\n",
      "Skipping line 26: expected 28 fields, saw 29\n",
      "Skipping line 27: expected 28 fields, saw 29\n",
      "Skipping line 28: expected 28 fields, saw 29\n",
      "Skipping line 29: expected 28 fields, saw 29\n",
      "Skipping line 30: expected 28 fields, saw 29\n",
      "Skipping line 31: expected 28 fields, saw 29\n",
      "Skipping line 32: expected 28 fields, saw 29\n",
      "Skipping line 33: expected 28 fields, saw 29\n",
      "Skipping line 34: expected 28 fields, saw 29\n",
      "Skipping line 35: expected 28 fields, saw 29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Reading a dataset and convert file name to corresponding number\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def preProcessData(csvFileName):\n",
    "    print(csvFileName+ \" will be preprocessed\")\n",
    "    data = pd.read_csv(csvFileName, error_bad_lines=False)\n",
    "    # data['number'] = data['filename'].str[:1]\n",
    "    #Dropping unnecessary columns\n",
    "    data = data.drop(['filename'],axis=1)\n",
    "    # data = data.drop(['label'],axis=1)\n",
    "    # data = data.drop(['chroma_stft'],axis=1)\n",
    "    data.shape\n",
    "\n",
    "    print(\"Preprocessing is finished\")\n",
    "    print(data.head())\n",
    "    return data\n",
    "\n",
    "trainData = preProcessData(TRAIN_CSV_FILE)\n",
    "testData = preProcessData(TEST_CSV_FILE)\n",
    "# moreTrainData = preProcessData(MORE_TRAIN_CSV_FILE)\n",
    "# moreTestData = preProcessData(MORE_TEST_CSV_FILE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2\n",
    "\n",
    "There are 50 recordings for each digit for each speaker: Jackson, Nicolas and Theo (total 1500 recordings)\n",
    "\n",
    "Training data has 49 recordings for each digit for each speaker: 1470 recordings total.\n",
    "Test data has 1 recordings for each digit for each speaker: 30 recordings total.\n",
    "\n",
    "The data used here comes from the recordings stored in:\n",
    "* ../data/recordings/train\n",
    "* ../data/recordings/test\n",
    "\n",
    "The model will be trained to predict the spoken digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPbfaLTrap87"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UEy6oG8RQmnN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y from training data: (76,)\n",
      "Y from validation data: (19,)\n",
      "Y from test data: (41,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into training, validation and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = np.array(trainData.iloc[:, :-1], dtype = float)\n",
    "y = trainData.iloc[:, -1]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_test = np.array(testData.iloc[:, :-1], dtype = float)\n",
    "y_test = testData.iloc[:, -1]\n",
    "\n",
    "print(\"Y from training data:\", y_train.shape)\n",
    "print(\"Y from validation data:\", y_val.shape)\n",
    "print(\"Y from test data:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S1o-OccqP5Ax",
    "outputId": "c93e0d6f-f5c0-4208-b0e5-7ecba885cddc"
   },
   "outputs": [],
   "source": [
    "# #Normalizing the dataset\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import numpy as np\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform( X_train )\n",
    "# X_val = scaler.transform( X_val )\n",
    "# X_test = scaler.transform( X_test )\n",
    "\n",
    "# print(\"X from training data\", X_train.shape)\n",
    "# print(\"X from validation data\", X_val.shape)\n",
    "# print(\"X from test data\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score with default hyperparameters: 0.6842\n"
     ]
    }
   ],
   "source": [
    "# import SVC classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# import metrics to compute accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# instantiate classifier with default hyperparameters\n",
    "svc=SVC() \n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred=svc.predict(X_val)\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_val, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score with rbf kernel and C=100.0 : 0.6316\n"
     ]
    }
   ],
   "source": [
    "# instantiate classifier with rbf kernel and C=100\n",
    "svc=SVC(C=10000.0) \n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred=svc.predict(X_val)\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with rbf kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_val, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score with linear kernel and C=1.0 : 0.7368\n",
      "Accuracy on training set: 0.645\n",
      "Accuracy on test set: 0.737\n"
     ]
    }
   ],
   "source": [
    "# instantiate classifier with linear kernel and C=1.0\n",
    "linear_svc=SVC(kernel='poly', C=1000.0) \n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "linear_svc.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred_test=linear_svc.predict(X_val)\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with linear kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_val, y_pred_test)))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(linear_svc.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(linear_svc.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forests\n",
      "Accuracy on training set: 0.974\n",
      "Accuracy on test set: 0.842\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=0).fit(X_train, y_train)\n",
    "print(\"\\nRandom Forests\")\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "pickle.dump(forest ,open('../trainedModel.sav' , 'wb'))\n",
    "model= pickle.load(open('../trainedModel.sav' , 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree\n",
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 0.789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#Train decision tree model\n",
    "tree = DecisionTreeClassifier(random_state=1).fit(X_train, y_train)\n",
    "print(\"\\nDecision Tree\")\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_val, y_val)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "sound-regnize-Keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "63963b3f4c440940f0b94a3100916033a226cb4f45979123153792d60aa56d6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
